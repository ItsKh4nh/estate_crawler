{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "import codecs\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from http.server import BaseHTTPRequestHandler, HTTPServer\n",
    "import urllib.parse as urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://alonhadat.com.vn/\"\n",
    "t = \"/can-ban-nha/trang-\"\n",
    "r = requests.get(link+t[0])\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "# print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_link(soup):\n",
    "    l = []\n",
    "    div_items = soup.find_all(\"div\", attrs={\"class\" : \"item\"})\n",
    "    for div_item in div_items:\n",
    "        a_links = div_item.find_all('a',href=True)\n",
    "        for a_link in a_links:\n",
    "            l.append(a_link['href'])\n",
    "    return l\n",
    "\n",
    "# for value in find_link(soup):\n",
    "#     print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_data(soup):\n",
    "    data = {}\n",
    "\n",
    "    # Tiêu đề\n",
    "    titles = soup.find_all('div', {\"class\": \"ct_title\"})\n",
    "    data['title'] = titles[0].text.strip() if titles else None\n",
    "\n",
    "    # Giá\n",
    "    prices = soup.find_all('div', {\"class\": \"ct_price\"})\n",
    "    data['price'] = prices[0].text.strip().replace(\"Gi\\u00e1:\",\"\").strip() if prices else None\n",
    "\n",
    "    # Diện tích\n",
    "    acreages = soup.find_all('div', {\"class\": \"ct_dt\"})\n",
    "    data['acreage'] = acreages[0].text.strip().replace(\"Di\\u1ec7n t\\u00edch:\",\"\").strip() if acreages else None\n",
    "\n",
    "    # Mô tả\n",
    "    descriptions = soup.find_all('div', {\"class\": \"ct_brief\"})\n",
    "    data['description'] = descriptions[0].text.strip().replace(\"\\r\\n\",\"\") if descriptions else None\n",
    "\n",
    "    # Ảnh\n",
    "    images = soup.find_all('div', {\"class\": \"thumbnail\"})\n",
    "    image_tag = images[0].find('img') if images else None\n",
    "    data['image_url'] = image_tag['src'] if image_tag and 'src' in image_tag.attrs else None\n",
    "    #link\n",
    "    b_links = soup.find_all('a', href=True)\n",
    "    data['link'] = b_links[0]['href'] if b_links else None\n",
    "           \n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def process_property_elements(soup):\n",
    "    data_list = []\n",
    "\n",
    "    # Tìm tất cả các phần tử HTML chứa thông tin\n",
    "    property_elements = soup.find_all('div', {\"class\": \"content-item\"})\n",
    "\n",
    "    for property_element in property_elements:\n",
    "        # Gọi hàm find_data cho mỗi phần tử và thêm kết quả vào danh sách\n",
    "        data_list.append(find_data(property_element))\n",
    "\n",
    "    return data_list\n",
    "\n",
    "def scrape_and_save(link, t, max_pages):\n",
    "    all_data = []\n",
    "\n",
    "    for i in range(1, max_pages + 1):\n",
    "        try:\n",
    "            r = requests.get(link + t + str(i)+\".htm\")\n",
    "            r.raise_for_status()  # Raise HTTPError for bad requests\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            \n",
    "            property_data = process_property_elements(soup)\n",
    "            all_data.extend(property_data)\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching page {i}: {e}\")\n",
    "\n",
    "    with open('alonhadat.json', 'w') as file:\n",
    "        json.dump(all_data, file)\n",
    "\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "max_pages = 50\n",
    "\n",
    "scrape_and_save(link, t, max_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\nconst fs = require('fs');\n\n//Đường dẫn đến tệp 'bat_dong_san.json'\nconst jsonFilePath = './alonhadat.json';\n\n// Đọc nội dung của tệp JSON\nfs.readFile(jsonFilePath, 'utf8', (err, data) => {\n  if (err) {\n    console.error(`Đã xảy ra lỗi khi đọc tệp JSON: ${err}`);\n    return;\n  }\n\n  // Chuyển đổi dữ liệu JSON thành đối tượng JavaScript\n  const propertyData = JSON.parse(data);\n\n  // In ra dữ liệu để xem kết quả\n  console.log(propertyData);\n});\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "const fs = require('fs');\n",
    "\n",
    "//Đường dẫn đến tệp 'bat_dong_san.json'\n",
    "const jsonFilePath = './alonhadat.json';\n",
    "\n",
    "// Đọc nội dung của tệp JSON\n",
    "fs.readFile(jsonFilePath, 'utf8', (err, data) => {\n",
    "  if (err) {\n",
    "    console.error(`Đã xảy ra lỗi khi đọc tệp JSON: ${err}`);\n",
    "    return;\n",
    "  }\n",
    "\n",
    "  // Chuyển đổi dữ liệu JSON thành đối tượng JavaScript\n",
    "  const propertyData = JSON.parse(data);\n",
    "\n",
    "  // In ra dữ liệu để xem kết quả\n",
    "  console.log(propertyData);\n",
    "});"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
